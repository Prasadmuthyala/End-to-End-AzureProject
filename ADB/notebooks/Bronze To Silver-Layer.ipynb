{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356a2b9d-624e-4fee-8561-4cf1f3b14722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa0406e-cc78-414a-9589-dfeccfb75d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"/mnt/bronzelayer/sampleDatasets/flights2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05c000a-b6d6-4855-9187-faa733623d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.display()\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/sampleDatasets/flights2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f761f458-cae3-4661-bdfc-39e62827b3d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09146e4b-7900-4e7c-bcdb-abde7eeea3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Get the current date in 'YYYY-MM-DD' format\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(f'dbfs:/mnt/bronzelayer/sampleDatasets/airport/')\n",
    "    if any(file.name.endswith('.csv') for file in files):\n",
    "        df2 = spark.read.format(\"csv\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .load(f\"/mnt/bronzelayer/sampleDatasets/airport/*.csv\")\n",
    "        # Split Description column to get the City and State\n",
    "        df2 = df2.withColumn(\"City\", split(col('Description'), ', ').getItem(0)) \\\n",
    "         .withColumn(\"State\", split(split(col('Description'), ',').getItem(1), ':').getItem(0))\\\n",
    "         .withColumn(\"name\", split(col('Description'), ':').getItem(1))\n",
    "        # df2.printSchema()\n",
    "        # display(df2)\n",
    "        df2.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/sampleDatasets/airport/{current_date}/\")\n",
    "    else:\n",
    "        display(\"No CSV files found\")\n",
    "except Exception as e:\n",
    "    display(f\"No data found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcd96594-abf3-44c2-9e03-fd5d5b3e2964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### json flight data\n",
    "Create Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff5c317-73a6-41cf-97db-d1d047bcfb55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5483c1-4551-4a2b-b870-6359296a10d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, TimestampType\n",
    "\n",
    "# Define the schema based on the JSON structure\n",
    "schema = StructType([\n",
    "    StructField(\"pagination\", StructType([\n",
    "        StructField(\"limit\", IntegerType(), True),\n",
    "        StructField(\"offset\", IntegerType(), True),\n",
    "        StructField(\"count\", IntegerType(), True),\n",
    "        StructField(\"total\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    \n",
    "    # Use ArrayType to handle the data field, which contains an array of flight records\n",
    "    StructField(\"data\", ArrayType(StructType([\n",
    "        StructField(\"flight_date\", StringType(), True),\n",
    "        StructField(\"flight_status\", StringType(), True),\n",
    "        StructField(\"departure\", StructType([\n",
    "            StructField(\"airport\", StringType(), True),\n",
    "            StructField(\"timezone\", StringType(), True),\n",
    "            StructField(\"iata\", StringType(), True),\n",
    "            StructField(\"icao\", StringType(), True),\n",
    "            StructField(\"terminal\", StringType(), True),\n",
    "            StructField(\"gate\", StringType(), True),\n",
    "            StructField(\"delay\", IntegerType(), True),\n",
    "            StructField(\"scheduled\", StringType(), True),\n",
    "            StructField(\"estimated\", StringType(), True),\n",
    "            StructField(\"actual\", StringType(), True),\n",
    "            StructField(\"estimated_runway\", StringType(), True),\n",
    "            StructField(\"actual_runway\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"arrival\", StructType([\n",
    "            StructField(\"airport\", StringType(), True),\n",
    "            StructField(\"timezone\", StringType(), True),\n",
    "            StructField(\"iata\", StringType(), True),\n",
    "            StructField(\"icao\", StringType(), True),\n",
    "            StructField(\"terminal\", StringType(), True),\n",
    "            StructField(\"gate\", StringType(), True),\n",
    "            StructField(\"baggage\", StringType(), True),\n",
    "            StructField(\"delay\", IntegerType(), True),\n",
    "            StructField(\"scheduled\", StringType(), True),\n",
    "            StructField(\"estimated\", StringType(), True),\n",
    "            StructField(\"actual\", StringType(), True),\n",
    "            StructField(\"estimated_runway\", StringType(), True),\n",
    "            StructField(\"actual_runway\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"airline\", StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"iata\", StringType(), True),\n",
    "            StructField(\"icao\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"flight\", StructType([\n",
    "            StructField(\"number\", StringType(), True),\n",
    "            StructField(\"iata\", StringType(), True),\n",
    "            StructField(\"icao\", StringType(), True),\n",
    "            StructField(\"codeshared\", StructType([\n",
    "                StructField(\"airline_name\", StringType(), True),\n",
    "                StructField(\"airline_iata\", StringType(), True),\n",
    "                StructField(\"airline_icao\", StringType(), True),\n",
    "                StructField(\"flight_number\", StringType(), True),\n",
    "                StructField(\"flight_iata\", StringType(), True),\n",
    "                StructField(\"flight_icao\", StringType(), True)\n",
    "            ]), True)\n",
    "        ]), True),\n",
    "        StructField(\"aircraft\", StringType(), True),\n",
    "        StructField(\"live\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "tmp=False\n",
    "try:\n",
    "    \n",
    "# Step 2: Read the JSON Data with the Defined Schema\n",
    "    json_file_path = f\"/mnt/bronzelayer/Api-response-raw/{current_date}/\"\n",
    "\n",
    "# Read the JSON data with the schema\n",
    "    df3 = spark.read.schema(schema).json(json_file_path)\n",
    "\n",
    "# Show the data\n",
    "    df3.display()\n",
    "    tmp=True\n",
    "except Exception as e:\n",
    "    display(f\"No data found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97df9b99-368d-40a4-a678-e2930dd4c153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "flatten Json data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a6a285-6a4d-4b38-8441-5ab041a77425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if tmp:\n",
    "    from pyspark.sql.functions import explode, col\n",
    "    from pyspark.sql.functions import split\n",
    "\n",
    "# Step 1: Explode the 'data' column to separate each flight record into its own row\n",
    "    df_exploded = df3.withColumn(\"flight\", explode(col(\"data\")))\n",
    "\n",
    "# Step 2: Flatten the nested fields from the 'flight' column into separate columns\n",
    "\n",
    "    df_flattened = df_exploded.select(\n",
    "        col(\"flight.flight_date\"),\n",
    "        col(\"flight.flight_status\"),\n",
    "    # Split 'departure.timezone' by '/' and select part of it\n",
    "        split(col(\"flight.departure.timezone\"), \"/\").alias(\"departure_timezone_parts\"),\n",
    "        col(\"flight.departure.airport\").alias(\"departure_airport\"),\n",
    "        col(\"flight.departure.iata\").alias(\"departure_iata\"),\n",
    "        col(\"flight.departure.icao\").alias(\"departure_icao\"),\n",
    "        col(\"flight.departure.terminal\").alias(\"departure_terminal\"),\n",
    "        col(\"flight.departure.gate\").alias(\"departure_gate\"),\n",
    "        col(\"flight.departure.delay\").alias(\"departure_delay\"),\n",
    "        col(\"flight.departure.scheduled\").alias(\"departure_scheduled\"),\n",
    "        col(\"flight.departure.estimated\").alias(\"departure_estimated\"),\n",
    "        col(\"flight.departure.actual\").alias(\"departure_actual\"),\n",
    "        col(\"flight.departure.estimated_runway\").alias(\"departure_estimated_runway\"),\n",
    "        col(\"flight.departure.actual_runway\").alias(\"departure_actual_runway\"),\n",
    "        col(\"flight.arrival.airport\").alias(\"arrival_airport\"),\n",
    "        split(col(\"flight.arrival.timezone\"),'/').alias(\"arrival_timezone_parts\"),\n",
    "        col(\"flight.arrival.iata\").alias(\"arrival_iata\"),\n",
    "        col(\"flight.arrival.icao\").alias(\"arrival_icao\"),\n",
    "        col(\"flight.arrival.terminal\").alias(\"arrival_terminal\"),\n",
    "        col(\"flight.arrival.gate\").alias(\"arrival_gate\"),\n",
    "        col(\"flight.arrival.baggage\").alias(\"arrival_baggage\"),\n",
    "        col(\"flight.arrival.delay\").alias(\"arrival_delay\"),\n",
    "        col(\"flight.arrival.scheduled\").alias(\"arrival_scheduled\"),\n",
    "        col(\"flight.arrival.estimated\").alias(\"arrival_estimated\"),\n",
    "        col(\"flight.arrival.actual\").alias(\"arrival_actual\"),\n",
    "        col(\"flight.arrival.estimated_runway\").alias(\"arrival_estimated_runway\"),\n",
    "        col(\"flight.arrival.actual_runway\").alias(\"arrival_actual_runway\"),\n",
    "        col(\"flight.airline.name\").alias(\"airline_name\"),\n",
    "        col(\"flight.airline.iata\").alias(\"airline_iata\"),\n",
    "        col(\"flight.airline.icao\").alias(\"airline_icao\"),\n",
    "        col(\"flight.flight.number\").alias(\"flight_number\"),\n",
    "        col(\"flight.flight.iata\").alias(\"flight_iata\"),\n",
    "        col(\"flight.flight.icao\").alias(\"flight_icao\"),\n",
    "        col(\"flight.flight.codeshared.airline_name\").alias(\"codeshared_airline_name\"),\n",
    "        col(\"flight.flight.codeshared.airline_iata\").alias(\"codeshared_airline_iata\"),\n",
    "        col(\"flight.flight.codeshared.airline_icao\").alias(\"codeshared_airline_icao\"),\n",
    "        col(\"flight.flight.codeshared.flight_number\").alias(\"codeshared_flight_number\"),\n",
    "        col(\"flight.flight.codeshared.flight_iata\").alias(\"codeshared_flight_iata\"),\n",
    "        col(\"flight.flight.codeshared.flight_icao\").alias(\"codeshared_flight_icao\")\n",
    "    )\n",
    "    df_transformed=df_flattened.withColumn(\"flight_date\", col(\"flight_date\").cast(\"date\"))\\\n",
    "                           .withColumn(\"departure_timezone_part1\", col(\"departure_timezone_parts\").getItem(0)).withColumn(\"departure_timezone_part2\", col(\"departure_timezone_parts\").getItem(1))\\\n",
    "                                .withColumn(\"arrival_timezone_part1\", col(\"arrival_timezone_parts\").getItem(0)).withColumn(\"arrival_timezone_part2\", col(\"arrival_timezone_parts\").getItem(1))\\\n",
    "                               .withColumn(\"departure_delay\", col(\"departure_delay\").cast(\"integer\"))\\\n",
    "                                .withColumn(\"arrival_delay\", col(\"arrival_delay\").cast(\"integer\"))\\\n",
    "                                    .withColumn(\"departure_scheduled\", split(col(\"departure_scheduled\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"departure_estimated\", split(col(\"departure_estimated\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"departure_actual\", split(col(\"departure_actual\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"arrival_scheduled\", split(col(\"arrival_scheduled\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"arrival_estimated\", split(col(\"arrival_estimated\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"arrival_actual\", split(col(\"arrival_actual\"), \"\\\\+\").getItem(0))\\\n",
    "                                    .withColumn(\"arrival_estimated_runway\", split(col(\"arrival_estimated_runway\"), \"//+\").getItem(0))\\\n",
    "                                    .withColumn(\"arrival_actual_runway\", split(col(\"arrival_actual_runway\"), \"//+\").getItem(0))\\\n",
    "                                    .withColumn(\"flight_number\", col(\"flight_number\").cast(\"integer\"))\n",
    "                                    \n",
    "                                                                       \n",
    "    df_transformed.display()\n",
    "    df_transformed.printSchema()\n",
    "    df_transformed.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/Api-response-raw/{current_date}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67f4875-b299-49ff-af54-fe80c8ed0279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###postgresql data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c14998c8-9e14-4a9d-b4b0-5d575d0bc19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  df3=spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(f\"/mnt/bronzelayer/postgreSQL-raw/{current_date}/\")\n",
    "  df3=df3.withColumn(\"movieId\", col(\"movieId\").cast(\"string\"))\\\n",
    "    .withColumn(\"num_ratings\", col(\"num_ratings\").cast(\"integer\"))\\\n",
    "    .withColumn(\"avg_rating\", col(\"avg_rating\").cast(\"double\"))\\\n",
    "      .withColumn(\"std_rating\", col(\"std_rating\").cast(\"double\"))\n",
    "  df3.printSchema()\n",
    "  df3.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/postgreSQL-raw/{current_date}/\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f63d0da-e431-4736-bd87-4f056c5ed373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Cancellation SQL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e735ddff-8249-4f31-8995-88f66dba363e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the SQL file\n",
    "df4 = spark.read.text(\"/mnt/bronzelayer/sampleDatasets/cancellation/\")  # Path to your SQL file\n",
    "\n",
    "# Step 2: Concatenate all lines into a single string\n",
    "queries = ''\n",
    "for line in df4.select(\"value\").collect():\n",
    "    queries += line[0].replace('\"', '') + \"\\n\"  # Add newline after each line for clarity\n",
    "\n",
    "# Step 3: Split the queries by semicolon (;) and execute them one by one\n",
    "queries_list = queries.split(';')\n",
    "\n",
    "# Step 4: Execute each query separately\n",
    "for query in queries_list:\n",
    "    query = query.strip()  # Clean up any extra spaces or empty queries\n",
    "    if query:  # Avoid executing empty queries\n",
    "        spark.sql(query)\n",
    "\n",
    "# Step 5: Verify the table creation\n",
    "result = spark.sql(\"SELECT * FROM Cancellation\")\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967ab785-4806-4dfd-a607-b8e31a3afe61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Get the current date in 'YYYY-MM-DD' format\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "result.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/sampleDatasets/cancellation/{current_date}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfba56c0-e1d2-4465-8fec-36bce56654e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###uniqueCarriers sql file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea107302-1ec1-49d1-9389-59155f8682fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the SQL file\n",
    "df5 = spark.read.text(\"/mnt/bronzelayer/sampleDatasets/uniqueCarriers/\")  # Path to your SQL file\n",
    "\n",
    "# Step 2: Concatenate all lines into a single string\n",
    "queries = ''\n",
    "for line in df5.select(\"value\").collect():\n",
    "    queries += line[0].replace('\"', '') + \"\\n\"  # Add newline after each line for clarity\n",
    "\n",
    "# Step 3: Split the queries by semicolon (;) and execute them one by one\n",
    "queries_list = queries.split(';')\n",
    "\n",
    "# Step 4: Execute each query separately\n",
    "for query in queries_list:\n",
    "    query = query.strip()  # Clean up any extra spaces or empty queries\n",
    "    if query:  # Avoid executing empty queries\n",
    "        # Check if the table already exists before attempting to create it\n",
    "        if \"CREATE TABLE\" in query:\n",
    "            table_name = query.split(\" \")[2]  # Extract table name from the CREATE statement\n",
    "            if not spark.catalog.tableExists(table_name):\n",
    "                spark.sql(query)\n",
    "            else:\n",
    "                print(f\"Table {table_name} already exists, skipping creation.\")\n",
    "        else:\n",
    "            spark.sql(query)\n",
    "\n",
    "# Step 5: Verify the table creation\n",
    "result = spark.sql(\"SELECT * FROM UNIQUE_CARRIERS\")\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac743dde-c8e2-4ce1-bc2b-d526e2e9c10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.write.format(\"delta\").mode(\"overwrite\").save(f\"/mnt/silverlayer/sampleDatasets/uniqueCarriers/{current_date}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8935cc85-5679-451e-8645-d28089ce716c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **<--THE END-->**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7783175085757601,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze To Silver-Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
